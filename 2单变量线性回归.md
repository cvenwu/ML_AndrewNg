# 单变量线性回归

## 模型描述

### 课程用到的一些符号说明
- m = 训练集的样本数量
- x = 输入变量 （或特征）
- y = 输出变量 (预测的值)
- (x , y) 表示一个训练样本
- $$(x_i, y_i)$$ 表示第i个训练样本


![单变量线性回归.png](./image/2单变量线性回归.png)

> 一般可以表示为$$h(x) = c_1 * x + c_2$$

## 代价函数

在线性回归预测中，我们希望获得两个理想的参数$$c_1和c_2$$使得我们预测的结果h与实际的结果y的差值越小越好。这里我们引入了代价函数$$\sum_{i=1}^m{(h-y)^2} $$， 也就是**预测值与实际值的误差平方和越小越好**, 也就是**最小化代价函数**

> 代价函数也被成为平方误差函数或平方误差代价函数

此时我们已经将问题转换为求使得代价函数$$\sum_{i=1}^m{(h-y)^2} $$最小的两个理想参数$$c_1和c_2$$， 我们可以把其看作线性回归的整体目标函数。

为了更直观理解，吴恩达老师将其代价函数简化为如下图中描述(即在求和的基础上除以2m)

![02代价函数.png](./image/02代价函数.png)
![02代价函数2.png](./image/02代价函数2.png)

也有其他评估线性回归的代价函数，但是平方误差和是我们最常见的一个评估手段。

## 梯度下降法(Gradient Descent Algorithm)
> 梯度下降法可以将代价函数最小化，不仅可以用来最小化代价函数，而且可以最小化其他函数，并且应用领域十分广泛

梯度下降的思路：
1. 给要确定值的所有参数一个初始值(通常我们都把他们初始化为0)
2. 不停的改变所有参数来减小代价函数的值直到找到了代价函数的最小值


> 计算注意事项：所有参数必须做到同时更新，不可以更新后赋值。梯度下降必须做到所有参数的同时更新。

![梯度下降公式以及注意事项](./image/梯度下降公式以及注意事项.png)
 

